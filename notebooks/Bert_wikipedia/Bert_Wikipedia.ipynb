{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8217d750",
   "metadata": {},
   "source": [
    "# BERT Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3634fb",
   "metadata": {},
   "source": [
    "Der BERT Tokenizer ist ein Tokenizer, der mit BERT arbeitet. Er hat viele Funktionen für alle Arten von Tokenisierungsaufgaben. Sie können den Tokenizer mit dieser Codezeile herunterladen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49a91100",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb10c0",
   "metadata": {},
   "source": [
    "Im Gegensatz zu den BERT-Modellen müssen Sie nicht für jede Art von Modell einen anderen Tokenizer herunterladen. Sie können denselben Tokenizer für alle verschiedenen BERT-Modelle verwenden, die hugging face bietet.\n",
    "Bei einer Texteingabe gehe ich im Allgemeinen so vor, dass ich sie in Projekten tokenisiere:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6581c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode_plus(text, add_special_tokens = True,\n",
    "                                 truncation = True, padding = \"max_length\", \n",
    "                                 return_attention_mask = True, \n",
    "                                 return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f1ca1a",
   "metadata": {},
   "source": [
    "Da BERT jeweils nur 512 Token als Eingabe akzeptieren/aufnehmen kann, muss der Parameter truncation auf True gesetzt werden. Der Parameter add special tokens dient nur dazu, dass BERT Token wie die Start-, End-, [SEP]- und [CLS]-Token hinzufügen kann. Return_tensors = \"pt\" dient nur dazu, dass der Tokenizer PyTorch-Tensoren zurückgibt. Wenn Sie das nicht wollen (vielleicht wollen Sie, dass er eine Liste zurückgibt), können Sie den Parameter entfernen und er gibt Listen zurück."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547c141",
   "metadata": {},
   "source": [
    "Im folgenden Code werden Sie sehen, dass ich nicht alle oben aufgeführten Parameter hinzugefügt habe. Das liegt in erster Linie daran, dass dies nicht notwendig ist, da ich den Text nicht für ein echtes Projekt tokenisiere. In einem echten maschinellen Lern-/NLP-Projekt sollten Sie diese Parameter hinzufügen, insbesondere die Trunkierung und das Auffüllen, da wir dies für jeden Stapel im Datensatz in einem echten Projekt tun müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbac9607",
   "metadata": {},
   "source": [
    "tokenizer.encode_plus() gibt speziell ein Wörterbuch mit Werten zurück und nicht nur eine Liste von Werten. Da tokenizer.encode_plus() viele verschiedene Arten von Informationen zurückgeben kann, wie z. B. die attention_masks und Token-Typ-IDs, wird alles in einem Wörterbuchformat zurückgegeben, und wenn Sie die spezifischen Teile der Kodierung abrufen möchten, können Sie dies wie folgt tun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a26456",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = encoding[\"input_ids\"][0]\n",
    "attention_mask = encoding[\"attention_mask\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d81a4",
   "metadata": {},
   "source": [
    "Da der Tokenizer außerdem ein Wörterbuch mit verschiedenen Werten zurückgibt, können wir, anstatt diese Werte wie oben gezeigt zu suchen und einzeln in das Modell zu übertragen, einfach die gesamte Kodierung wie folgt übergeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34402ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(**encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51927121",
   "metadata": {},
   "source": [
    "Ein weiterer wichtiger Aspekt des Tokenizers ist, dass Sie angeben können, dass bestimmte Token abgerufen werden sollen, falls gewünscht. Wenn Sie beispielsweise eine maskierte Sprachmodellierung durchführen und eine Maske an einer Stelle einfügen möchten, die Ihr Modell dekodieren soll, dann können Sie das Maskentoken einfach wie folgt abrufen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_token = tokenizer.mask_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e4810",
   "metadata": {},
   "source": [
    "und Sie können es einfach in Ihre Eingabe einfügen, indem Sie es mit Ihrem Eingabetext verketten.\n",
    "Auf die gleiche Weise können Sie auch viele andere Token abrufen, wie z. B. das [SEP]-Token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b542e",
   "metadata": {},
   "source": [
    "Normalerweise verwende ich die Funktion tokenizer.encode_plus(), um meine Eingaben zu tokenisieren, aber es gibt noch eine weitere Funktion, die verwendet werden kann, um Eingaben zu tokenisieren, und zwar tokenizer.encode(). Hier ist ein Beispiel dafür:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48b2910",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(text, return_tensors = \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6e12da",
   "metadata": {},
   "source": [
    "Der Hauptunterschied zwischen tokenizer.encode_plus() und tokenizer.encode() ist, dass tokenizer.encode_plus() mehr Informationen zurückgibt. Insbesondere werden die tatsächlichen Eingabe-Ids, die Aufmerksamkeitsmasken und die Token-Typ-Ids zurückgegeben, und zwar in einem Wörterbuch. tokenizer.encode() gibt nur die Eingabe-Ids zurück, und zwar entweder als Liste oder als Tensor, je nach dem Parameter return_tensors = \"pt\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f45453",
   "metadata": {},
   "source": [
    "# Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15a80f",
   "metadata": {},
   "source": [
    "Masked Language Modeling ist die Aufgabe, ein maskiertes Token in einem Satz zu dekodieren. Vereinfacht ausgedrückt ist es die Aufgabe, die Lücken zu füllen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4609a8c8",
   "metadata": {},
   "source": [
    "Anstatt einfach nur das beste Kandidatenwort für die Ersetzung des Masken-Tokens zu ermitteln, zeige ich Ihnen, wie Sie die 10 besten Ersetzungswörter für das Masken-Token ermitteln können, und zwar folgendermaßen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cce8f12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c857862cc04e8b840e1748a05cc8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/420M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France, paris, contains the Eiffel Tower.\n",
      "The capital of France, lyon, contains the Eiffel Tower.\n",
      "The capital of France, lille, contains the Eiffel Tower.\n",
      "The capital of France, toulouse, contains the Eiffel Tower.\n",
      "The capital of France, marseille, contains the Eiffel Tower.\n",
      "The capital of France, orleans, contains the Eiffel Tower.\n",
      "The capital of France, strasbourg, contains the Eiffel Tower.\n",
      "The capital of France, nice, contains the Eiffel Tower.\n",
      "The capital of France, cannes, contains the Eiffel Tower.\n",
      "The capital of France, versailles, contains the Eiffel Tower.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',    return_dict = True)\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \", contains the Eiffel Tower.\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "output = model(**input)\n",
    "logits = output.logits\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_10 = torch.topk(mask_word, 10, dim = 1)[1][0]\n",
    "for token in top_10:\n",
    "   word = tokenizer.decode([token])\n",
    "   new_sentence = text.replace(tokenizer.mask_token, word)\n",
    "   print(new_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2125a37b",
   "metadata": {},
   "source": [
    "Hugging Face ist so aufgebaut, dass Sie für die Aufgaben, für die es bereits trainierte Modelle hat, dieses spezifische Modell herunterladen/importieren müssen. In diesem Fall müssen wir das Modell \"Bert For Masked Language Modeling\" herunterladen, während der Tokenizer für alle verschiedenen Modelle gleich ist, wie ich bereits im obigen Abschnitt erwähnt habe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c51085",
   "metadata": {},
   "source": [
    "Bei der maskierten Sprachmodellierung wird ein Maskentoken an der gewünschten Stelle eingefügt, an der Sie das beste Kandidatenwort vorhersagen möchten, das an dieser Stelle stehen würde. Sie können das Masken-Token einfach einfügen, indem Sie es an der gewünschten Position in Ihrer Eingabe verketten, wie ich es oben getan habe. Das Bert-Modell für maskierte Sprachmodellierung sagt das beste Wort/Token in seinem Vokabular voraus, das dieses Wort ersetzen würde. Die Logits sind die Ausgabe des BERT-Modells, bevor eine Softmax-Aktivierungsfunktion auf die Ausgabe von BERT angewendet wird. Um die Logits zu erhalten, müssen wir bei der Initialisierung des Modells in den Parametern return_dict = True angeben, sonst führt der obige Code zu einem Kompilierungsfehler. Nachdem wir die Eingabekodierung an das BERT-Modell übergeben haben, können wir die Logits einfach durch Angabe von output.logits erhalten, das einen Tensor zurückgibt, und danach können wir schließlich eine Softmax-Aktivierungsfunktion auf die Logits anwenden. Durch die Anwendung einer Softmax auf die Ausgabe von BERT erhalten wir probabilistische Verteilungen für jedes der Wörter im BERT-Wortschatz. Wörter mit einem höheren Wahrscheinlichkeitswert sind bessere Kandidaten für die Ersetzung des Maskentokens. Um den Tensor der Softmax-Werte aller Wörter im BERT-Vokabular für die Ersetzung des Maskentokens zu erhalten, können wir den Index des maskierten Tokens angeben, den wir mit torch.where() erhalten. Da ich in diesem Beispiel die 10 besten Kandidaten für die Ersetzung des maskierten Tokens abrufe (Sie können mehr als 10 erhalten, indem Sie den Parameter entsprechend anpassen), habe ich die Funktion torch.topk() verwendet, mit der Sie die obersten k Werte in einem gegebenen Tensor abrufen können, und sie gibt einen Tensor mit diesen obersten k Werten zurück. Danach wird der Prozess relativ einfach, da wir nur durch den Tensor iterieren und das Masken-Token im Satz durch das Kandidaten-Token ersetzen müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbb8ac",
   "metadata": {},
   "source": [
    "Sie können sehen, dass Paris tatsächlich der Top-Kandidat für die Ersetzung des Maskentokens ist.\n",
    "Wenn Sie nur das Top-Kandidatenwort erhalten möchten, können Sie dies tun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e80c8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased',    return_dict = True)\n",
    "text = \"The capital of France, \" + tokenizer.mask_token + \" ,contains the Eiffel Tower.\"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "mask_index = torch.where(input[\"input_ids\"][0] == tokenizer.mask_token_id)\n",
    "logits = model(**input)\n",
    "logits = logits.logits\n",
    "softmax = F.softmax(logits, dim = -1)\n",
    "mask_word = softmax[0, mask_index, :]\n",
    "top_word = torch.argmax(mask_word, dim=1)\n",
    "print(tokenizer.decode(top_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a597ae",
   "metadata": {},
   "source": [
    "Anstatt torch.topk() zum Abrufen der 10 höchsten Werte zu verwenden, benutzen wir einfach torch.argmax(), das den Index des höchsten Wertes im Tensor zurückgibt. Der Rest des Codes ist so ziemlich das Gleiche wie der Originalcode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ec0ac5",
   "metadata": {},
   "source": [
    "# Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc67ad4b",
   "metadata": {},
   "source": [
    "Bei der Sprachmodellierung geht es darum, das beste Wort für die Fortsetzung eines Satzes aus allen bereits im Satz enthaltenen Wörtern zu ermitteln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2231fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, BertLMHeadModel\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertLMHeadModel.from_pretrained('bert-base-uncased',\n",
    "return_dict=True, is_decoder = True)\n",
    "text = \"A knife is very \"\n",
    "input = tokenizer.encode_plus(text, return_tensors = \"pt\")\n",
    "output = model(**input).logits[:, -1, :]\n",
    "softmax = F.softmax(output, -1)\n",
    "index = torch.argmax(softmax, dim = -1)\n",
    "x = tokenizer.decode(index)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93407891",
   "metadata": {},
   "source": [
    "Die Sprachmodellierung funktioniert ganz ähnlich wie die maskierte Sprachmodellierung. Zu Beginn müssen wir das spezielle Bert Language Model Head Model herunterladen, das im Wesentlichen ein BERT-Modell mit einem Sprachmodellierungskopf darauf ist. Ein zusätzlicher Parameter, den wir bei der Instanziierung dieses Modells angeben müssen, ist der Parameter is_decoder = True. Wir müssen diesen Parameter angeben, wenn wir dieses Modell als eigenständiges Modell für die Vorhersage des nächstbesten Wortes in der Sequenz verwenden wollen. Der Rest des Codes ist relativ identisch mit dem der maskierten Sprachmodellierung: Wir müssen die Logits des Modells abrufen, aber anstatt den Index des maskierten Tokens anzugeben, müssen wir nur die Logits des letzten versteckten Zustands des Modells nehmen (unter Verwendung des Index -1), die Softmax dieser Logits berechnen, den größten Wahrscheinlichkeitswert im Vokabular finden und dieses Token dekodieren und ausgeben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676b053f",
   "metadata": {},
   "source": [
    "# Next Sentence Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc165bb4",
   "metadata": {},
   "source": [
    "Die Vorhersage des nächsten Satzes ist die Aufgabe, vorherzusagen, ob ein Satz auf einen anderen Satz folgt. Hier ist mein Code für diese Aufgabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54f445a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9953, 0.0047]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForNextSentencePrediction\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')\n",
    "prompt = \"The child came home from school.\"\n",
    "next_sentence = \"He played soccer after school.\"\n",
    "encoding = tokenizer.encode_plus(prompt, next_sentence, return_tensors='pt')\n",
    "outputs = model(**encoding)[0]\n",
    "softmax = F.softmax(outputs, dim = 1)\n",
    "print(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d03a2",
   "metadata": {},
   "source": [
    "Bei der Vorhersage des nächsten Satzes geht es darum, vorherzusagen, wie gut ein Satz als nächster Satz für einen gegebenen Satz geeignet ist. In diesem Fall ist \"Das Kind kam von der Schule nach Hause.\" der gegebene Satz und wir versuchen vorherzusagen, ob \"Er spielte nach der Schule Fußball.\" der nächste Satz ist. Zu diesem Zweck fügt der BERT-Tokenizer automatisch ein [SEP]-Token zwischen den Sätzen ein, das die Trennung zwischen den beiden Sätzen darstellt, und das spezielle Bert-Modell für die Vorhersage des nächsten Satzes sagt zwei Werte dafür voraus, ob der Satz der nächste Satz ist. Bert gibt zwei Werte in einem Tensor zurück: Der erste Wert gibt an, ob der zweite Satz eine Fortsetzung des ersten ist, und der zweite Wert gibt an, ob der zweite Satz eine zufällige Folge oder keine gute Fortsetzung des ersten ist. Anders als bei der Sprachmodellierung rufen wir keine Logits ab, weil wir nicht versuchen, eine Softmax auf dem Vokabular von BERT zu berechnen; wir versuchen einfach, eine Softmax auf den beiden Werten zu berechnen, die BERT für die Vorhersage des nächsten Satzes zurückgibt, damit wir sehen können, welcher Wert den höchsten Wahrscheinlichkeitswert hat, und dieser wird darstellen, ob der zweite Satz eine gute Fortsetzung des ersten ist. Sobald wir die Softmax-Werte erhalten haben, können wir uns den Tensor ansehen, indem wir ihn ausdrucken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b3b3d",
   "metadata": {},
   "source": [
    "Da der erste Wert deutlich höher ist als der zweite Index, geht das BERT davon aus, dass der zweite Satz auf den ersten Satz folgt, was die richtige Antwort ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247b71",
   "metadata": {},
   "source": [
    "# Extractive Question Answering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d633aeb",
   "metadata": {},
   "source": [
    "Bei der extraktiven Beantwortung von Fragen geht es darum, eine Frage in einem bestimmten Kontext zu beantworten, indem man die Anfangs- und Endindizes ausgibt, wo die Antwort im Kontext liegt. Hier ist mein Code für die extraktive Beantwortung von Fragen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b310a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'softmax'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d9/1b92tl_13g32z4j72swk9lh40000gn/T/ipykernel_4282/1075570651.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mstart_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mend_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m## add one ##because of python list indexing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_max\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mend_max\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'softmax'"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering\n",
    "import torch\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "question = \"What is the capital of France?\"\n",
    "text = \"The capital of France is Paris.\"\n",
    "inputs = tokenizer.encode_plus(question, text, return_tensors='pt')\n",
    "start, end = model(**inputs)\n",
    "start_max = torch.argmax(F.softmax(start, dim = -1))\n",
    "end_max = torch.argmax(F.softmax(end, dim = -1)) + 1 ## add one ##because of python list indexing\n",
    "answer = tokenizer.decode(inputs[\"input_ids\"][0][start_max : end_max])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbdff1",
   "metadata": {},
   "source": [
    "Ähnlich wie bei den anderen drei Aufgaben laden wir zunächst das spezielle BERT-Modell für die Beantwortung von Fragen herunter und tokenisieren unsere beiden Eingaben: die Frage und den Kontext. Im Gegensatz zu den anderen Modellen ist der Prozess für dieses Modell relativ einfach, da es die Werte für jedes Wort in der tokenisierten Eingabe ausgibt. Wie bereits erwähnt, funktioniert die extraktive Beantwortung von Fragen durch die Berechnung der besten Start- und Endindizes für die Position der Antwort im Kontext. Das Modell gibt für alle Wörter im Kontext/Eingabe Werte zurück, die angeben, wie gut sie als Start- und Endwert für die gegebene Frage geeignet sind; mit anderen Worten, jedes der Wörter in der Eingabe erhält einen Start- und Endindexwert, der angibt, ob es ein gutes Startwort für die Antwort oder ein gutes Endwort für die Antwort wäre. Der Rest dieses Prozesses ist ziemlich ähnlich zu dem, was wir bei den anderen drei Programmen gemacht haben; wir berechnen die Softmax dieser Punktzahlen, um die probabilistische Verteilung der Werte zu finden, ermitteln die höchsten Werte für die Start- und End-Tensoren mit torch.argmax() und finden die tatsächlichen Token, die diesem Start- und Endbereich in der Eingabe entsprechen, und dekodieren sie und geben sie aus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b074d",
   "metadata": {},
   "source": [
    "# Using BERT for any task you want"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33919e16",
   "metadata": {},
   "source": [
    "Obwohl die Textzusammenfassung, die Beantwortung von Fragen und ein grundlegendes Sprachmodell besonders wichtig sind, möchten viele Menschen BERT für andere, nicht spezifizierte Aufgaben verwenden, insbesondere in der Forschung. Dazu werden die Rohdaten der gestapelten Kodierer von BERT genommen und ein eigenes spezifisches Modell, meist eine lineare Schicht, angehängt, das dann für den jeweiligen Datensatz fein abgestimmt wird. Wenn Sie dies in Pytorch mit der Hugging Face Transformer-Bibliothek tun, ist es am besten, dies als ein Pytorch Deep Learning-Modell wie dieses einzurichten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "class Bert_Model(nn.Module):\n",
    "   def __init__(self, class):\n",
    "       super(Bert_Model, self).__init__()\n",
    "       self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "       self.out = nn.Linear(self.bert.config.hidden_size, classes)\n",
    "   def forward(self, input):\n",
    "       _, output = self.bert(**input)\n",
    "       out = self.out(output)\n",
    "       return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed944fe8",
   "metadata": {},
   "source": [
    "Wie Sie sehen können, habe ich, anstatt ein spezielles BERT-Modell herunterzuladen, das bereits für eine bestimmte Aufgabe wie die Beantwortung von Fragen entwickelt wurde, das unbearbeitete, vortrainierte BertModel heruntergeladen, dem keine Köpfe zugewiesen sind.\n",
    "Um die Größe der rohen BERT-Ausgaben zu erhalten, verwenden Sie einfach self.bert.config.hidden_size und fügen Sie diese der Anzahl der Klassen hinzu, die Ihre lineare Schicht ausgeben soll."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796a49ea",
   "metadata": {},
   "source": [
    "Um den obigen Code für die Sentiment-Analyse zu verwenden, was überraschenderweise eine Aufgabe ist, die in der Hugging-Face-Transformer-Bibliothek nicht heruntergeladen/bereits erledigt ist, können Sie einfach eine Sigmoid-Aktivierungsfunktion am Ende der linearen Schicht hinzufügen und die Klassen auf 1 setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc45eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "class Bert_Model(nn.Module):\n",
    "   def __init__(self, class):\n",
    "       super(Bert_Model, self).__init__()\n",
    "       self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "       self.out = nn.Linear(self.bert.config.hidden_size, classes)\n",
    "       self.sigmoid = nn.Sigmoid()\n",
    "   def forward(self, input, attention_mask):\n",
    "       _, output = self.bert(input, attention_mask = attention_mask)\n",
    "       out = self.sigmoid(self.out(output))\n",
    "       return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
